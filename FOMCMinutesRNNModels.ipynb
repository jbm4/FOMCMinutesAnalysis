{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "FOMCMinutesModels",
      "provenance": [],
      "collapsed_sections": [
        "SmoNGOTw3s4h",
        "sGWil4l43xZO"
      ],
      "mount_file_id": "1WE_m1Bz3QghJqHGtUeQv6PkGBS0H9Bbz",
      "authorship_tag": "ABX9TyO9pAwq8KBSrlSvI4v1UT7S",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jbm4/FOMCMinutesAnalysis/blob/main/FOMCMinutesRNNModels.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yduVACBNHcdJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb614698-df9a-460f-da62-d51946b54321"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import pickle\n",
        "import spacy\n",
        "import re\n",
        "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.pipeline import Pipeline\n",
        "from nltk.stem.snowball import SnowballStemmer\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mUahRog3NMD8",
        "outputId": "2438cc8a-12b6-4e73-a7dc-7a89a7358fa5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQJHQh60PdUj"
      },
      "source": [
        "merged_df=pd.read_csv('/content/drive/MyDrive/thinkful/Capstone 4/merged_df.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tipnvANHT1sc"
      },
      "source": [
        "unrate=pd.read_csv('/content/drive/MyDrive/thinkful/Capstone 4/UNRATE.csv')\n",
        "cpiaucsl=pd.read_csv('/content/drive/MyDrive/thinkful/Capstone 4/CPIAUCSL.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZBjOyUPVLEm"
      },
      "source": [
        "unrate.DATE=pd.to_datetime(unrate['DATE'])\n",
        "cpiaucsl.DATE=pd.to_datetime(cpiaucsl.DATE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PDIoaLs4RGBm"
      },
      "source": [
        "datetimes=['datetime_min', 'datetime_fed', 'next_month', 'DATE']\n",
        "for dt in datetimes:\n",
        "  merged_df[dt]=pd.to_datetime(merged_df[dt])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "algKszr4V54C"
      },
      "source": [
        "merged_df=merged_df.merge(unrate, how='inner', on='DATE')\n",
        "merged_df=merged_df.merge(cpiaucsl, how='inner', on='DATE')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "id": "C_lXV9CsmQ33",
        "outputId": "703219e6-966f-4a22-c029-5e6d20c7a85a"
      },
      "source": [
        "merged_df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>date</th>\n",
              "      <th>filename</th>\n",
              "      <th>raw_text</th>\n",
              "      <th>preprocessed</th>\n",
              "      <th>datetime_min</th>\n",
              "      <th>next_month</th>\n",
              "      <th>DATE</th>\n",
              "      <th>FEDFUNDS</th>\n",
              "      <th>datetime_fed</th>\n",
              "      <th>change</th>\n",
              "      <th>change_cat</th>\n",
              "      <th>change_binary</th>\n",
              "      <th>change_1</th>\n",
              "      <th>change_binary_1</th>\n",
              "      <th>UNRATE</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>19560417</td>\n",
              "      <td>fomchistmin19560417.txt</td>\n",
              "      <td>A meeting of the Federal Open Market Committee...</td>\n",
              "      <td>meet feder open market committe held offic boa...</td>\n",
              "      <td>1956-04-17</td>\n",
              "      <td>1956-05-01</td>\n",
              "      <td>1956-05-01</td>\n",
              "      <td>2.75</td>\n",
              "      <td>1956-05-01</td>\n",
              "      <td>0.13</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0</td>\n",
              "      <td>4.3</td>\n",
              "      <td>27.03</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       date                 filename  ... UNRATE CPIAUCSL\n",
              "0  19560417  fomchistmin19560417.txt  ...    4.3    27.03\n",
              "\n",
              "[1 rows x 16 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmoNGOTw3s4h"
      },
      "source": [
        "## Doc2Vec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "saVAFKApRIfn"
      },
      "source": [
        "#encoding with Doc2Vec\n",
        "\n",
        "documents=[TaggedDocument(doc, [i]) for i, doc in enumerate(merged_df.raw_text)]\n",
        "model=Doc2Vec(documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5i1pVOJ7SoOa"
      },
      "source": [
        "doc2vec=pd.DataFrame([[document]+list(model[document]) for document in range(len(documents))]).drop(0, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-jN0arjaZsNt",
        "outputId": "572100b4-ea12-4fa4-ed85-6e0145a72202"
      },
      "source": [
        "doc2vec.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(822, 100)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KmbJTHyVaQpw"
      },
      "source": [
        "doc2vec[['change_binary','datetime_min', 'datetime_fed']]=merged_df[['change_binary','datetime_min', 'datetime_fed']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHyEG0tEa2zk"
      },
      "source": [
        "doc2vec_1993=doc2vec[doc2vec['datetime_min'] > '1993-01-01']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pwrkg5vUfM4z"
      },
      "source": [
        "#random forest classifier model\n",
        "x_train1993, x_test1993, y_train1993, y_test1993 = train_test_split(doc2vec_1993.drop(columns=['change_binary','datetime_min', 'datetime_fed']), doc2vec_1993['change_binary'], test_size=0.3)\n",
        "\n",
        "rfc_model = RandomForestClassifier(n_estimators=75, min_samples_leaf=4, max_depth=12)\n",
        "\n",
        "rfc_model.fit(x_train1993, y_train1993)\n",
        "rfc_model_predictions = rfc_model.predict(x_test1993)\n",
        "print(classification_report(y_test1993, rfc_model_predictions))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYKtHvU9r1pE",
        "outputId": "0c02ce2e-1b3e-4183-a5f8-7334ded6b755"
      },
      "source": [
        "rfc_model_predictions = rfc_model.predict(x_test1993)\n",
        "print(classification_report(y_test1993, rfc_model_predictions))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.78      0.93      0.85        60\n",
            "           1       0.33      0.11      0.17        18\n",
            "\n",
            "    accuracy                           0.74        78\n",
            "   macro avg       0.56      0.52      0.51        78\n",
            "weighted avg       0.68      0.74      0.69        78\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWT1BNt4sRkX",
        "outputId": "7a23b383-3299-4265-bed9-9586bdd5b6dc"
      },
      "source": [
        "doc2vec_1993.change_binary.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    184\n",
              "1     74\n",
              "Name: change_binary, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ARbdtT0Ktj1w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGWil4l43xZO"
      },
      "source": [
        "## Bert"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnHQ4XG9Aens"
      },
      "source": [
        "bert_df=merged_df[['filename', 'raw_text', 'datetime_min', 'datetime_fed', 'change', 'change_cat', 'change_1']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fRebgIja3yvB"
      },
      "source": [
        "def text_cleaner(text):\n",
        "  # Visual inspection identifies a form of punctuation that spaCy doesn't\n",
        "  # recognize: the double dash --. Better get rid of it now!\n",
        "  # text = re.sub(r'--',' ',text)\n",
        "  # text = re.sub(\"[\\[].*?[\\]]\", \"\", text)\n",
        "  # text = re.sub(r\"(\\b|\\s+\\-?|^\\-?)(\\d+|\\d*\\.\\d+)\\b\", \" \", text)\n",
        "  # text = ' '.join(text.split())\n",
        "  text= re.sub(\"Mr.\", 'Mr', text)\n",
        "  text= re.sub(\"Mrs.\", 'Mrs', text)\n",
        "\n",
        "  return text\n",
        "\n",
        "nlp = spacy.load('en')\n",
        "nlp.add_pipe(nlp.create_pipe('sentencizer')) # updated\n",
        "\n",
        "#think about only keeping the sentences if it has a verb\n",
        "def sentences(text):\n",
        "  doc = nlp(text)\n",
        "  sentences = [sent.string.strip() for sent in doc.sents]\n",
        "  return sentences\n",
        "def remove_segments(sentence_list):\n",
        "  return [s for s in sentence_list if len(s.split())>2 ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trrGXu6mFk9v",
        "outputId": "e99548a9-7c17-47a2-cd1b-1e7bbb677796"
      },
      "source": [
        "bert_df['cleaned']=bert_df['raw_text'].apply(lambda x: sentences(text_cleaner(x)))\n",
        "bert_df['cleaned']=bert_df['cleaned'].apply(lambda x: remove_segments(x))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \"\"\"Entry point for launching an IPython kernel.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qzJr3h6DXJeP"
      },
      "source": [
        "#pickling for later\n",
        "import pickle\n",
        "\n",
        "#bert_df.to_pickle('/content/drive/MyDrive/thinkful/Capstone 4/merged_df.pkl')\n",
        "bert_df=pickle.load(open('/content/drive/MyDrive/thinkful/Capstone 4/merged_df.pkl', 'rb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVW4WhcuJ-qB"
      },
      "source": [
        "#only working with 1993 and onward info for now\n",
        "bert1993_df=bert_df[bert_df['datetime_min']>'1993-01-01']\n",
        "\n",
        "#bert1993e_df=bert1993_df.explode('cleaned')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        },
        "id": "0jXjq2VmhzQZ",
        "outputId": "d9c11d34-f80c-4cfe-830b-6ec975314410"
      },
      "source": [
        "bert_df.head(2)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>raw_text</th>\n",
              "      <th>datetime_min</th>\n",
              "      <th>datetime_fed</th>\n",
              "      <th>change</th>\n",
              "      <th>change_cat</th>\n",
              "      <th>change_1</th>\n",
              "      <th>cleaned</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>fomchistmin19560417.txt</td>\n",
              "      <td>A meeting of the Federal Open Market Committee...</td>\n",
              "      <td>1956-04-17</td>\n",
              "      <td>1956-05-01</td>\n",
              "      <td>0.13</td>\n",
              "      <td>1</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>[A meeting of the Federal Open Market Committe...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fomcminutes20090603.txt</td>\n",
              "      <td>Minutes of the Federal Open Market Committee \\...</td>\n",
              "      <td>2009-06-03</td>\n",
              "      <td>2009-07-01</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>[Minutes of the Federal Open Market Committee ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  filename  ...                                            cleaned\n",
              "0  fomchistmin19560417.txt  ...  [A meeting of the Federal Open Market Committe...\n",
              "1  fomcminutes20090603.txt  ...  [Minutes of the Federal Open Market Committee ...\n",
              "\n",
              "[2 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mnxxp2WqwhDr"
      },
      "source": [
        "**Creating the model to embed the sentences**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WfdxPDlmIXPI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1gjzR_PE2gy"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "!pip install transformers\n",
        "import transformers as ppb\n",
        "# For DistilBERT:\n",
        "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
        "\n",
        "## Want BERT instead of distilBERT? Uncomment the following line:\n",
        "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
        "\n",
        "# Load pretrained model/tokenizer\n",
        "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
        "model = model_class.from_pretrained(pretrained_weights)\n",
        "\n",
        "def embed_document(sentences_list):\n",
        "  sentences_df=pd.DataFrame(sentences_list)\n",
        "  tokenized = sentences_df[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "\n",
        "  max_len = 0\n",
        "  for i in tokenized.values:\n",
        "    if len(i) > max_len:\n",
        "      max_len = len(i)\n",
        "\n",
        "  padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\n",
        "  attention_mask = np.where(padded != 0, 1, 0)\n",
        "\n",
        "  input_ids = torch.tensor(padded) \n",
        "  attention_mask2 = torch.tensor(attention_mask)\n",
        "\n",
        "  with torch.no_grad():\n",
        "    last_hidden_states = model(input_ids, attention_mask=attention_mask2)\n",
        "  features = last_hidden_states[0][:,0,:].numpy()\n",
        "\n",
        "  return features\n",
        "def CNN(features):\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xXmAv1zxHjBC"
      },
      "source": [
        "bert1993_df['embedded']=bert1993_df['cleaned'].apply(lambda x: embed_document(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yCTgPTyrmK6R"
      },
      "source": [
        "bert1993_df.to_pickle('/content/drive/MyDrive/thinkful/Capstone 4/bert1993_df.pkl')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_54XUd_qJoLv"
      },
      "source": [
        "tokenized = bert1993e_df['cleaned'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
        "\n",
        "max_len = 0\n",
        "for i in tokenized.values:\n",
        "  if len(i) > max_len:\n",
        "    max_len = len(i)\n",
        "\n",
        "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
        "\n",
        "attention_mask = np.where(padded != 0, 1, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YueJUwpbihIk"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "#saving padded and attention mask\n",
        "\n",
        "#np.save('/content/drive/MyDrive/thinkful/Capstone 4/attention_mask.npy', attention_mask)\n",
        "#np.save('/content/drive/MyDrive/thinkful/Capstone 4/padded.npy', padded)\n",
        "attention_mask=np.load('/content/drive/MyDrive/thinkful/Capstone 4/attention_mask.npy')\n",
        "padded=np.load('/content/drive/MyDrive/thinkful/Capstone 4/padded.npy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kiGpnOjgNElU"
      },
      "source": [
        "input_ids = torch.tensor(padded) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MwgzEA5hqQ3l"
      },
      "source": [
        "attention_mask2 = torch.tensor(attention_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIZW621pqqCW"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "#np.save('/content/drive/MyDrive/thinkful/Capstone 4/attention_mask2.npy', attention_mask2)\n",
        "#np.save('/content/drive/MyDrive/thinkful/Capstone 4/input_ids.npy', input_ids)\n",
        "#attention_mask2=np.load('/content/drive/MyDrive/thinkful/Capstone 4/attention_mask2.npy')\n",
        "#input_ids=np.load('/content/drive/MyDrive/thinkful/Capstone 4/input_ids.npy')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcpsKghBjbJd"
      },
      "source": [
        "with torch.no_grad():\n",
        "  last_hidden_states = model(input_ids, attention_mask=attention_mask2)\n",
        "\n",
        "#features = last_hidden_states[0][:,0,:].numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pBLpqwI6qfrw",
        "outputId": "89977725-3d94-48ee-954c-6c61a9bb29cc"
      },
      "source": [
        "attention_mask2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       ...,\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0],\n",
              "       [1, 1, 1, ..., 0, 0, 0]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhCGSbBWqgHt"
      },
      "source": [
        "#for use with bert\n",
        "\n",
        "#TODO: representing the documents with a CNN\n",
        "#tfidf_docs=pd.concat([tfidf_df, TFID1993_df[['filename']].reset_index(drop=True)], axis=1)\n",
        "model=Sequential()\n",
        "\n",
        "#first hidden layer\n",
        "model.add(Conv2D(32,(3,3), input_shape=input_shape))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#the above are part of the same layer\n",
        "#stride is 1 by default\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuTy7YRbAheP"
      },
      "source": [
        "## RNN with TFID transformer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PfisS025Ak4z"
      },
      "source": [
        "TFID1993_df=merged_df[merged_df['datetime_min']>'1993-01-01'][['filename',\t'raw_text', \n",
        "                                                              'datetime_min',\t'FEDFUNDS',\t\n",
        "                                                              'datetime_fed',\t'change',\t\n",
        "                                                              'change_binary',\t'change_1',\t\n",
        "                                                              'change_binary_1', 'UNRATE',\t\n",
        "                                                              'CPIAUCSL']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 97
        },
        "id": "iF0dJUOuQ7Od",
        "outputId": "c241c6da-b903-470f-8c8d-e81e56fa6987"
      },
      "source": [
        "TFID1993_df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>raw_text</th>\n",
              "      <th>datetime_min</th>\n",
              "      <th>FEDFUNDS</th>\n",
              "      <th>datetime_fed</th>\n",
              "      <th>change</th>\n",
              "      <th>change_binary</th>\n",
              "      <th>change_1</th>\n",
              "      <th>change_binary_1</th>\n",
              "      <th>UNRATE</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fomcminutes20090603.txt</td>\n",
              "      <td>Minutes of the Federal Open Market Committee \\...</td>\n",
              "      <td>2009-06-03</td>\n",
              "      <td>0.16</td>\n",
              "      <td>2009-07-01</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>214.726</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  filename  ... CPIAUCSL\n",
              "1  fomcminutes20090603.txt  ...  214.726\n",
              "\n",
              "[1 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SErkFksNBoY7"
      },
      "source": [
        "#could use this to filter out short sentences or sentences without verbs:\n",
        "#sent_text = nltk.sent_tokenize(text)\n",
        "\n",
        "def preprocess(doc):\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    stemmer = SnowballStemmer('english')\n",
        "    \n",
        "    tokenized = word_tokenize(doc)\n",
        "\n",
        "    cleaned = [stemmer.stem(lemmatizer.lemmatize(token.lower())) \n",
        "               for token in tokenized \n",
        "               if not token.lower() in stopwords.words('english') \n",
        "               if token.isalnum()] #should I make this also include numbers?\n",
        "               \n",
        "\n",
        "    untokenized = \" \".join(cleaned)\n",
        "    #preprocessed.append(untokenized)\n",
        "        \n",
        "    return untokenized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zm48Q9JRA3q"
      },
      "source": [
        "TFID1993_df['lemmatized']=TFID1993_df['raw_text'].apply(lambda x: preprocess(x))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 148
        },
        "id": "p_osfLXaRKbh",
        "outputId": "39031388-b3b7-4296-b371-483267aa9753"
      },
      "source": [
        "#TFID1993_df.to_pickle('/content/drive/MyDrive/thinkful/Capstone 4/TFID1993_df.pkl')\n",
        "#TFID1993_df=pickle.load(open('/content/drive/MyDrive/thinkful/Capstone 4/TFID1993_df.pkl', 'rb'))\n",
        "\n",
        "TFID1993_df.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>filename</th>\n",
              "      <th>raw_text</th>\n",
              "      <th>datetime_min</th>\n",
              "      <th>FEDFUNDS</th>\n",
              "      <th>datetime_fed</th>\n",
              "      <th>change</th>\n",
              "      <th>change_binary</th>\n",
              "      <th>change_1</th>\n",
              "      <th>change_binary_1</th>\n",
              "      <th>UNRATE</th>\n",
              "      <th>CPIAUCSL</th>\n",
              "      <th>lemmatized</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>fomcminutes20090603.txt</td>\n",
              "      <td>Minutes of the Federal Open Market Committee \\...</td>\n",
              "      <td>2009-06-03</td>\n",
              "      <td>0.16</td>\n",
              "      <td>2009-07-01</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0</td>\n",
              "      <td>-0.05</td>\n",
              "      <td>0</td>\n",
              "      <td>9.5</td>\n",
              "      <td>214.726</td>\n",
              "      <td>minut feder open market committe june 2009 joi...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                  filename  ...                                         lemmatized\n",
              "1  fomcminutes20090603.txt  ...  minut feder open market committe june 2009 joi...\n",
              "\n",
              "[1 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEgzJf2zai4F"
      },
      "source": [
        "#getting the vector representation\n",
        "\n",
        "vectorizer = TfidfVectorizer(\n",
        "    max_df=0.5, min_df=2, use_idf=True, norm=u'l2', smooth_idf=True)\n",
        "\n",
        "\n",
        "# Applying the vectorizer\n",
        "X = vectorizer.fit_transform(TFID1993_df[\"lemmatized\"])\n",
        "tfidf_df = pd.DataFrame(X.toarray(), columns=vectorizer.get_feature_names())\n",
        "tfidf_df_1=tfidf_df.shift()\n",
        "tfidf_merge = pd.concat([tfidf_df, tfidf_df_1], axis=1)\n",
        "\n",
        "tfidf_merge = pd.concat([tfidf_merge, TFID1993_df.reset_index()], axis=1)\n",
        "tfidf_merge.tail(1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJvbkIvlckjE"
      },
      "source": [
        "tfidf_merge=tfidf_merge.dropna()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9puqtxejx69"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Embedding,Dropout\n",
        "from tensorflow.keras.layers import LSTM\n",
        "from tensorflow.keras.layers import concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.models import Sequential\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YhjSQgx1_QP2",
        "outputId": "2dd2c1d3-d317-4636-a61a-203bef4a71a4"
      },
      "source": [
        "tfidf_df.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(258, 4836)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IfhVXOoJ4fmW"
      },
      "source": [
        "#TODO: X and y have a different number of rows\n",
        "X=pd.concat([tfidf_df, tfidf_df_1], axis=1).dropna()\n",
        "y=tfidf_merge.reset_index(drop=True)['change_binary']\n",
        "X_train, y_train, X_test, y_test = train_test_split(X, y, test_size=.25, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EN1pcJKCk6e1",
        "outputId": "ed4108bd-9d2a-41c7-bd4e-5c3d628c03c3"
      },
      "source": [
        "n_timesteps, n_features = X_train.shape[0], X_train.shape[1]\n",
        "n_outputs=1 #y_train[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(LSTM(100, input_shape=(n_timesteps, n_features)))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(n_outputs, activation='tanh'))\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_5 (LSTM)                (None, 100)               3909200   \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 100)               10100     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 3,919,401\n",
            "Trainable params: 3,919,401\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZzYB2oyl_6b"
      },
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 409
        },
        "id": "nBnRMeKN7gOb",
        "outputId": "adf4e09d-54bb-41b3-f5a6-60614e442227"
      },
      "source": [
        "batch_size=80\n",
        "epochs = 50\n",
        "\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, verbose=1) #batch_size=batch_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-78-b5d0562cccc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#batch_size=batch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1061\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1062\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1063\u001b[0;31m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[1;32m   1064\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1065\u001b[0m       \u001b[0;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1115\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1116\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1117\u001b[0;31m         model=model)\n\u001b[0m\u001b[1;32m   1118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1119\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mds_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    280\u001b[0m             label, \", \".join(str(i.shape[0]) for i in nest.flatten(data)))\n\u001b[1;32m    281\u001b[0m       \u001b[0mmsg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"Please provide data which shares the same first dimension.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Data cardinality is ambiguous:\n  x sizes: 192\n  y sizes: 65\nPlease provide data which shares the same first dimension."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BnyalyfdmENe"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import one_hot\n",
        "\n",
        "vocab_size = len(set(yelp.normalize.apply(lambda x: x.strip().split(' ')).sum())) # np.sum(yelp.text_norm.str.len())\n",
        "yelp['encoded'] = yelp.text_norm.apply(one_hot, args=[vocab_size])\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "independent_vars = pad_sequences(yelp['encoded'].values)\n",
        "independent_vars\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "target = yelp.sentiment\n",
        "X_train, X_test, y_train, y_test = train_test_split(independent_vars, target, test_size=0.2, random_state=21)\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, SimpleRNN\n",
        "\n",
        "max_words = np.max(X_train)+1\n",
        "max_len = X_train.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(max_words, 100, input_length=max_len))\n",
        "model.add(SimpleRNN(32, activation=\"relu\"))\n",
        "model.add(Dense(8, activation=\"relu\"))\n",
        "model.add(Dense(1, activation=\"sigmoid\"))\n",
        "model.summary()\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "epochs = 50\n",
        "batch_size = 128\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test),\n",
        "          batch_size=batch_size, epochs=epochs)\n",
        "\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}